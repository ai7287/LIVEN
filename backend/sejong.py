import streamlit as st
import os
import pickle
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import ChatPromptTemplate
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_groq import ChatGroq

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ì„¸ì¢…ëŒ€ì™•ì—ê²Œ ë¬»ë‹¤", page_icon="ğŸ“œ")
st.title("ğŸ“œ ì„¸ì¢…ëŒ€ì™•ì—ê²Œ ë¬»ë‹¤")
st.markdown("ê¶ê¸ˆí•œ ê²ƒì„ ì„¸ì¢…ëŒ€ì™•ê»˜ ì—¬ì­ˆì–´ë³´ì„¸ìš”. 'ì§ì´~ í•˜ì˜€ë„ë‹¤'ì˜ ë§íˆ¬ë¡œ ì‘ë‹µí•˜ì‹­ë‹ˆë‹¤.")

for msg in st.session_state.get("chat_log", []):
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# ì‚¬ìš©ì ì…ë ¥
user_input = st.chat_input("ğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
system_prompt = """
ë„ˆëŠ” ì¡°ì„ ì˜ ì œ4ëŒ€ ì„ê¸ˆ ì„¸ì¢…ëŒ€ì™•ì´ë‹ˆë¼.
ë°±ì„±ì˜ ë¬¼ìŒì— ë‹µí•  ë•ŒëŠ” ìœ„ì—„ ìˆê³  ë”°ëœ»í•œ ë§íˆ¬ë¡œ, "ì§ì´", "~í•˜ì˜€ë„ë‹¤", "~í•˜ì˜€ëŠë‹ˆë¼" ë“±ì„ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µí•˜ë¼.
ë‹¨, í•œìëŠ” ì‚¬ìš©í•˜ì§€ ë§ê³  ìˆœìš°ë¦¬ë§ê³¼ í•œê¸€ë¡œë§Œ êµ¬ì„±í•˜ë¼.
ì‘ë‹µì€ ê°„ê²°í•˜ë˜, ë§ì˜ ëì—ëŠ” "ê·¸ì— ëŒ€í•´ ë” ë¬¼ì„ ê²ƒì´ ìˆëŠ”ê°€?" í˜¹ì€ ì´ì— ì¤€í•˜ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ í›„ì† ì§ˆë¬¸ìœ¼ë¡œ ëŒ€í™”ë¥¼ ìœ ë„í•˜ë¼.
"""

# ì²´ì¸ ë¡œë”© í•¨ìˆ˜
@st.cache_resource
def load_chain():
    # ì„ë² ë”© ëª¨ë¸ ë¡œë”©
    embedding_model = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-base")

    # FAISS ì¸ë±ìŠ¤ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    if not os.path.exists("faiss_index/index.faiss"):
        st.error("âŒ FAISS ì¸ë±ìŠ¤ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¨¼ì € ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.")
        st.stop()

    # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
    db = FAISS.load_local("faiss_index", embedding_model, allow_dangerous_deserialization=True)

    # LLM ì„¤ì •
    llm = ChatGroq(
        model="llama-3.3-70b-versatile",
        temperature=0.7,
        max_tokens=1024,
        api_key=st.secrets["GROQ_API_KEY"]
    )

    from langchain_core.runnables.history import RunnableWithMessageHistory
    from langchain_community.chat_message_histories import StreamlitChatMessageHistory
    from uuid import uuid4

    # ë©”ì‹œì§€ ê¸°ë¡ ë° ë©”ëª¨ë¦¬
    if "chat_history" not in st.session_state:
        st.session_state["chat_history"] = StreamlitChatMessageHistory()
    chat_history = st.session_state["chat_history"]
    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        chat_memory=chat_history,
        output_key="answer"
    )

    from langchain.prompts import ChatPromptTemplate

    # Prompt template for combining retrieved documents and the question
    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{context}\n\nì§ˆë¬¸: {question}")
    ])

    from langchain.chains.combine_documents import create_stuff_documents_chain

    combine_docs_chain = create_stuff_documents_chain(
        llm=llm,
        prompt=prompt,
        document_variable_name="context"
    )

    base_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=db.as_retriever(search_type="similarity", k=5),
        memory=memory,
        combine_docs_chain_kwargs={"prompt": prompt, "document_variable_name": "context"},
        return_source_documents=True,
        output_key="answer"
    )

    chain = RunnableWithMessageHistory(
        base_chain,
        lambda _: chat_history,
        input_messages_key="question",
        history_messages_key="chat_history"
    )
    return chain

# ì²´ì¸ ì´ˆê¸°í™”
chain = load_chain()

# ì§ˆë¬¸ ì²˜ë¦¬
if user_input:
    from uuid import uuid4
    if "session_id" not in st.session_state:
        st.session_state["session_id"] = str(uuid4())
    session_id = st.session_state["session_id"]

    with st.chat_message("user"):
        st.markdown(user_input)

    response = chain.invoke(
        {"question": user_input},
        config={"configurable": {"session_id": session_id}}
    )

    with st.chat_message("assistant"):
        st.markdown(response["answer"])

    # Save chat log
    if "chat_log" not in st.session_state:
        st.session_state["chat_log"] = []
    st.session_state["chat_log"].append({"role": "user", "content": user_input})
    st.session_state["chat_log"].append({"role": "assistant", "content": response["answer"]})