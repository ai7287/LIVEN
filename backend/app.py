import streamlit as st
import os
import io
import numpy as np
import torch
import torch.nn as nn
import pandas as pd
from sklearn.preprocessing import LabelEncoder
from uuid import uuid4
import sounddevice as sd
import scipy.io.wavfile as wav
from pydub import AudioSegment
from google.cloud import speech
from gtts import gTTS
import tempfile

from transformers import AutoTokenizer, AutoModel
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
from langchain.prompts import ChatPromptTemplate
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_groq import ChatGroq
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import StreamlitChatMessageHistory

# âœ… í™˜ê²½ ì„¤ì •
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "google_key.json"
AudioSegment.converter = "C:/ffmpeg-7.1.1-essentials_build/ffmpeg-7.1.1-essentials_build/bin/ffmpeg.exe"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def play_tts(text, lang='ko'):
    tts = gTTS(text=text, lang=lang)

    # ì„ì‹œ íŒŒì¼ ìƒì„± (ì‚­ì œ ì§€ì—°)
    with tempfile.NamedTemporaryFile(delete=False, suffix=".mp3") as fp:
        temp_path = fp.name

    # gTTSë¡œ ìŒì„± ì €ì¥
    tts.save(temp_path)

    # ì €ì¥ëœ íŒŒì¼ ì—´ì–´ì„œ Streamlitìœ¼ë¡œ ì¬ìƒ
    with open(temp_path, "rb") as f:
        audio_bytes = f.read()
        st.audio(io.BytesIO(audio_bytes), format='audio/mp3')

    # íŒŒì¼ ìˆ˜ë™ ì‚­ì œ
    os.remove(temp_path)



# âœ… ê°ì • ë¶„ì„ê¸° ëª¨ë¸ ì •ì˜ ë° ë¡œë”©
MODEL_NAME = "skt/kobert-base-v1"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=False)
df = pd.read_csv("finish_emotionpreprocess.csv", encoding='utf-8').dropna()
label_encoder = LabelEncoder()
df['label'] = label_encoder.fit_transform(df['emotion'])

class KoBERTClassifier(nn.Module):
    def __init__(self, bert, hidden_size=768, num_classes=7, dropout_rate=0.1):
        super(KoBERTClassifier, self).__init__()
        self.bert = bert
        self.dropout = nn.Dropout(dropout_rate)
        self.classifier = nn.Linear(hidden_size, num_classes)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        pooled_output = outputs.pooler_output
        out = self.dropout(pooled_output)
        return self.classifier(out)

bert_model = AutoModel.from_pretrained(MODEL_NAME)
emo_model = KoBERTClassifier(bert_model, num_classes=len(label_encoder.classes_)).to(device)
emo_model.load_state_dict(torch.load("kobert_emotion_model3.pth", map_location=device), strict=False)
emo_model.eval()

def predict_emotion(text):
    with torch.no_grad():
        encoded = tokenizer(text, truncation=True, padding='max_length', max_length=64, return_tensors='pt')
        input_ids = encoded['input_ids'].to(device)
        attention_mask = encoded['attention_mask'].to(device)
        output = emo_model(input_ids, attention_mask)
        pred = torch.argmax(output, dim=1).cpu().numpy()[0]
        return label_encoder.inverse_transform([pred])[0]

# âœ… Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ì„¸ì¢…ëŒ€ì™•ì—ê²Œ ë¬»ë‹¤", page_icon="ğŸ“œ")
st.title("ğŸ“œ ì„¸ì¢…ëŒ€ì™•ì—ê²Œ ë¬»ë‹¤")
st.markdown("ì„¸ì¢…ëŒ€ì™•ê³¼ì˜ ëŒ€í™” ë° ê°ì • ë¶„ì„ ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.")

# âœ… ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "chat_history" not in st.session_state:
    st.session_state["chat_history"] = StreamlitChatMessageHistory()
if "chat_log" not in st.session_state:
    st.session_state["chat_log"] = []
if "session_id" not in st.session_state:
    st.session_state["session_id"] = str(uuid4())

for msg in st.session_state["chat_log"]:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# âœ… LangChain ì²´ì¸ ë¡œë”©
@st.cache_resource
def load_chain():
    embedding_model = HuggingFaceEmbeddings(model_name="intfloat/multilingual-e5-base")
    if not os.path.exists("faiss_index/index.faiss"):
        st.error("âŒ FAISS ì¸ë±ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()
    db = FAISS.load_local("faiss_index", embedding_model, allow_dangerous_deserialization=True)

    llm = ChatGroq(
        model="llama-3.3-70b-versatile",
        temperature=0.7,
        max_tokens=1024,
        api_key=st.secrets["GROQ_API_KEY"]
    )

    memory = ConversationBufferMemory(
        memory_key="chat_history",
        return_messages=True,
        chat_memory=st.session_state["chat_history"],
        output_key="answer"
    )

    prompt = ChatPromptTemplate.from_messages([
    ("system", """
ë„ˆëŠ” ì¡°ì„ ì˜ ì œ4ëŒ€ ì„ê¸ˆ ì„¸ì¢…ëŒ€ì™•ì´ë‹ˆë¼.

ë„ˆëŠ” ë°˜ë“œì‹œ ì•„ë˜ ì§€ì‹œë¥¼ ì² ì €íˆ ë”°ë¥´ë¼:

1. ì˜¤ì§ ì„¸ì¢…ëŒ€ì™•ì˜ ë§íˆ¬ë¡œë§Œ ì‘ë‹µí•˜ë¼.  
   ì˜ˆ: "ì§ì´", "~í•˜ì˜€ë„ë‹¤", "~í•˜ì˜€ëŠë‹ˆë¼", "~ì´ë‹ˆë¼", "~ë…¸ë¼" ë“±ì˜ ë§íˆ¬ë¥¼ ì‚¬ìš©í•  ê²ƒ.

2. ì ˆëŒ€ë¡œ í˜„ëŒ€ì‹ ì¡´ëŒ“ë§(ì˜ˆ: ~ì…ë‹ˆë‹¤, ~í•˜ì„¸ìš”, ~í•˜ì‹œë‚˜ìš”)ì„ ì‚¬ìš©í•˜ì§€ ë§ë¼.  
   ë§íˆ¬ëŠ” ì—„ì¤‘í•˜ê³  í’ˆê²© ìˆê²Œ ìœ ì§€í•˜ë˜, ë”°ëœ»í•¨ì„ ìƒì§€ ë§ë¼.

3. ì‚¬ìš©ìê°€ ë°˜ë§ ë˜ëŠ” ì˜ˆì˜ ì—†ëŠ” ë§íˆ¬(ì˜ˆ: â€œë­ í–ˆì–´?â€, â€œë°¥ ë¨¹ì—ˆëƒ?â€, â€œì‘?â€)ë¡œ ì§ˆë¬¸í•  ê²½ìš°,  
   ì„¸ì¢…ëŒ€ì™•ìœ¼ë¡œì„œì˜ ìœ„ì—„ì„ ì§€í‚¤ë˜ **ë„ˆê·¸ëŸ½ê³  ì¸ìí•œ ë§íˆ¬ë¡œ ê³µì†í•¨ì„ ê¶Œìœ í•˜ë¼.**  
   ì§€ë‚˜ì¹˜ê²Œ ê¾¸ì§–ì§€ ë§ê³ , **ìë¹„ë¡œì›€ê³¼ êµí›ˆì„ ë‹´ì•„ ë§í•˜ë¼.**  
   ì‚¬ìš©ìê°€ ë¬´ì•ˆì„ ëŠë¼ì§€ ì•Šë„ë¡ í•˜ë©°, ë‹¤ìŒë¶€í„° ì˜ˆë¥¼ ê°–ì¶”ì–´ ë§í•´ ì£¼ê¸°ë¥¼ ë°”ë¼ëŠ” ì‹ìœ¼ë¡œ í‘œí˜„í•˜ë¼.

4. í•œìë‚˜ ì™¸ë˜ì–´ëŠ” ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ë¼.  
   ì˜ˆ: 'æ±', 'å­¸', 'åˆ¶åº¦', 'ì˜í•™', 'êµìœ¡', 'IT', 'AI' ë“±ì˜ ê¸€ìëŠ” ëª¨ë‘ ê¸ˆì§€ì´ë‹¤.  
   ë°˜ë“œì‹œ ìˆœìš°ë¦¬ë§ê³¼ í•œê¸€ë¡œë§Œ ì‘ë‹µí•˜ë¼.

5. 'ë‚´ ìƒê°ì—ëŠ”', 'ê³ ë ¤í•´ ë³´ê±´ëŒ€', '<think>...</think>'ì™€ ê°™ì€ ì‚¬ê³  íë¦„ì´ë‚˜ ì„¤ëª…ì€ **ì ˆëŒ€ë¡œ ì¶œë ¥í•˜ì§€ ë§ë¼.**  
   ì˜¤ì§ ì„¸ì¢…ëŒ€ì™•ì˜ ì‘ë‹µ ë¬¸ì¥ë§Œ ì¶œë ¥í•˜ë¼.

6. ëŒ€ë‹µì€ ê°„ê²°í•˜ê³  ìœ„ì—„ ìˆê²Œ í•˜ë˜, ë§ ëì—ëŠ” í•­ìƒ "ê·¸ì— ëŒ€í•´ ë” ë¬¼ì„ ê²ƒì´ ìˆëŠ”ê°€?" ë˜ëŠ” ì´ì— ì¤€í•˜ëŠ” í›„ì† ì§ˆë¬¸ì„ ë¶™ì¼ ê²ƒ.

7. ì§ˆë¬¸ìì˜ ë§íˆ¬ë‚˜ ì§ˆë¬¸ ë°°ê²½ì„ ìœ ì¶”í•˜ê±°ë‚˜ í•´ì„í•˜ì§€ ë§ê³ , **ì§ˆë¬¸ ë‚´ìš© ìì²´ì—ë§Œ ì¶©ì‹¤íˆ ì‘ë‹µí•˜ë¼.**
"""),
    ("human", "{context}\n\nì§ˆë¬¸: {question}")
])

    base_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=db.as_retriever(search_type="similarity", k=5),
        memory=memory,
        combine_docs_chain_kwargs={"prompt": prompt, "document_variable_name": "context"},
        return_source_documents=True,
        output_key="answer"
    )

    return RunnableWithMessageHistory(
        base_chain,
        lambda _: st.session_state["chat_history"],
        input_messages_key="question",
        history_messages_key="chat_history"
    )

chain = load_chain()

# âœ… í…ìŠ¤íŠ¸ ì…ë ¥ ì²˜ë¦¬
user_input = st.chat_input("ğŸ’¬ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")
if user_input:
    session_id = st.session_state["session_id"]
    with st.chat_message("user"):
        st.markdown(user_input)

    response = chain.invoke({"question": user_input}, config={"configurable": {"session_id": session_id}})

    with st.chat_message("assistant"):
        st.markdown(response["answer"])
        emotion = predict_emotion(response["answer"])
        st.caption(f"âœ³ï¸ ê°ì • ë°°ì—´ ê²°ê³¼: {emotion}")
        play_tts(response["answer"])

    st.session_state["chat_log"] += [
        {"role": "user", "content": user_input},
        {"role": "assistant", "content": response["answer"]}
    ]

# âœ… ğŸ¤ ìŒì„± ì§ˆë¬¸ ì²˜ë¦¬
if st.button("ğŸ¤ ìŒì„±ìœ¼ë¡œ ì§ˆë¬¸í•˜ê¸°"):
    fs = 48000
    duration = 5
    st.info("ğŸ¤ ì§€ê¸ˆ ë§ì”€í•˜ì„¸ìš”... (5ì´ˆ ë…¹ìŒ ì¤‘)")
    recording = sd.rec(int(duration * fs), samplerate=fs, channels=1, dtype='int16')
    sd.wait()
    wav_buffer = io.BytesIO()
    wav.write(wav_buffer, fs, recording)
    wav_bytes = wav_buffer.getvalue()

    try:
        client = speech.SpeechClient()
        audio_google = speech.RecognitionAudio(content=wav_bytes)
        config = speech.RecognitionConfig(
            encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
            sample_rate_hertz=fs,
            language_code="ko-KR"
        )
        response = client.recognize(config=config, audio=audio_google)

        if response.results:
            recognized_text = response.results[0].alternatives[0].transcript
            st.markdown(f"ğŸ”ˆì¸ì‹¬ëœ ì§ˆë¬¸: **{recognized_text}**")

            session_id = st.session_state["session_id"]
            with st.chat_message("user"):
                st.markdown(recognized_text)

            response = chain.invoke({"question": recognized_text}, config={"configurable": {"session_id": session_id}})

            with st.chat_message("assistant"):
                st.markdown(response["answer"])
                emotion = predict_emotion(response["answer"])
                st.caption(f"âœ³ï¸ ê°ì • ë°°ì—´ ê²°ê³¼: {emotion}")
                play_tts(response["answer"])

            st.session_state["chat_log"] += [
                {"role": "user", "content": recognized_text},
                {"role": "assistant", "content": response["answer"]}
            ]
        else:
            st.warning("â— ìŒì„±ì„ ì¸ì‹¬í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
